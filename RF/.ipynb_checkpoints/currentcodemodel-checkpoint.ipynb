{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d17bfd-c497-4b72-af7d-70c475b2d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, root_mean_squared_error\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "# === 1. Create output directories ===\n",
    "output_folder_common = Path(\"output_common\")\n",
    "output_folder_speed = Path(\"output_speed_based\")\n",
    "output_folder_other = Path(\"output_other_features\")\n",
    "output_folder_common.mkdir(exist_ok=True)\n",
    "output_folder_speed.mkdir(exist_ok=True)\n",
    "output_folder_other.mkdir(exist_ok=True)\n",
    "\n",
    "# === 2. Load & clean data ===\n",
    "df = pd.read_csv(\"main dataset.csv\")\n",
    "df = df.dropna(subset=['crash_rate_per_km'])\n",
    "osm_ids = df['osm_id']\n",
    "df = df.drop(columns=['city','osm_id','name'])\n",
    "\n",
    "# === 3. Bin rare categories for high-cardinality 'surface' ===\n",
    "bin_cols = ['surface']\n",
    "threshold = 5\n",
    "for col in bin_cols:\n",
    "    freqs = df[col].value_counts()\n",
    "    df[col] = df[col].where(freqs[df[col]] >= threshold, 'other')\n",
    "\n",
    "# === 4. Feature engineering ===\n",
    "# Interaction terms\n",
    "df['aadt_curv_interaction'] = df['aadt'] * df['mean_curvature']\n",
    "df['aadt_lanes_interaction'] = df['aadt'] * df['lanes']\n",
    "df['aadt_length_interaction'] = df['aadt'] * df['segment_length_km']\n",
    "df['aadt_speed'] = df['aadt'] * df['maxspeed']\n",
    "df['aadt_pop_interaction'] = df['aadt'] * df['population']\n",
    "df['lanes_length_interaction'] = df['lanes'] * df['segment_length_km']\n",
    "df['lanes_speed_interaction'] = df['lanes'] * df['maxspeed']\n",
    "df['curv_lanes_interaction'] = df['mean_curvature'] * df['lanes']\n",
    "\n",
    "# Polynomial/nonlinear terms\n",
    "df['maxspeed_sq'] = df['maxspeed'] ** 2\n",
    "df['mean_curvature_sq'] = df['mean_curvature'] ** 2\n",
    "df['aadt_sq'] = df['aadt'] ** 2\n",
    "df['lanes_sq'] = df['lanes'] ** 2\n",
    "df['segment_length_sq'] = df['segment_length_km'] ** 2\n",
    "\n",
    "# Ratios/densities (protect against divide-by-zero)\n",
    "df['aadt_per_lane'] = df['aadt'] / df['lanes'].replace(0, np.nan)\n",
    "df['pop_density_per_length'] = df['population'] / df['segment_length_km'].replace(0, np.nan)\n",
    "df['aadt_per_length'] = df['aadt'] / df['segment_length_km'].replace(0, np.nan)\n",
    "df['crashes_per_pop'] = df['city_total_crashes'] / df['population'].replace(0, np.nan)\n",
    "df['crashes_per_aadt'] = df['city_total_crashes'] / df['aadt'].replace(0, np.nan)\n",
    "\n",
    "# Binary/categorical combinations (example: adjust according to your categories)\n",
    "df['urban'] = (df['classification'].str.lower().str.contains('urban')).astype(int)\n",
    "df['urban_high_aadt'] = ((df['urban'] == 1) & (df['aadt'] > df['aadt'].median())).astype(int)\n",
    "df['bridge_high_speed'] = ((df['bridge'] == 1) & (df['maxspeed'] > df['maxspeed'].median())).astype(int)\n",
    "df['tunnel_high_speed'] = ((df['tunnel'] == 1) & (df['maxspeed'] > df['maxspeed'].median())).astype(int)\n",
    "\n",
    "# If you have 'sidewalk' and 'oneway' as binary\n",
    "df['sidewalk_high_pop'] = ((df['sidewalk'] == 1) & (df['population'] > df['population'].median())).astype(int)\n",
    "df['oneway_high_speed'] = ((df['oneway'] == 1) & (df['maxspeed'] > df['maxspeed'].median())).astype(int)\n",
    "\n",
    "def attach_ids(df_part, ids):\n",
    "    df_part = df_part.reset_index(drop=True)\n",
    "    df_part['osm_id'] = ids.reset_index(drop=True)\n",
    "    return df_part\n",
    "\n",
    "# === 5. Train/test split ===\n",
    "y_rate = df['crash_rate_per_km']\n",
    "y_crashes = df['city_total_crashes']\n",
    "X = df.drop(columns=['crash_rate_per_km','city_total_crashes','segment_length'])\n",
    "a,b,c,d,e,f,g,h = train_test_split(\n",
    "    X, y_rate, y_crashes, osm_ids,\n",
    "    test_size=0.2, stratify=X['highway'], random_state=42\n",
    ")\n",
    "X_train, X_test, y_tr_rate, y_te_rate, y_tr_crash, y_te_crash, osm_tr, osm_te = (a,b,c,d,e,f,g,h)\n",
    "X_train = attach_ids(X_train, osm_tr)\n",
    "X_test = attach_ids(X_test, osm_te)\n",
    "for y in [y_tr_rate, y_tr_crash, y_te_rate, y_te_crash]: y.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# observed speed bounds\n",
    "s_min, s_max = df['maxspeed'].min(), df['maxspeed'].max()\n",
    "\n",
    "# === 6. Preprocessing pipeline (minimal, robust against collinearity) ===\n",
    "\n",
    "binary_cols = ['bridge', 'tunnel', 'lit', 'sidewalk', 'oneway', 'cycleway']\n",
    "\n",
    "numeric_cols = [\n",
    "    # Base features\n",
    "    'lanes',\n",
    "    'maxspeed',\n",
    "    'segment_width',\n",
    "    'mean_curvature',\n",
    "    'aadt',\n",
    "    'population',\n",
    "    'segment_length_km',\n",
    "\n",
    "  \n",
    "    'aadt_curv_interaction',     \n",
    "    'aadt_per_lane',              \n",
    "\n",
    "    # Binary columns (as numeric)\n",
    "    'bridge',\n",
    "    'tunnel',\n",
    "    'lit',\n",
    "    'sidewalk',\n",
    "    'oneway',\n",
    "    'cycleway',\n",
    "    'urban'\n",
    "]\n",
    "pre = ColumnTransformer([\n",
    "    ('num', StandardScaler(), numeric_cols),\n",
    "    ('cat', OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False), ['surface', 'highway', 'classification'])\n",
    "], remainder='drop')\n",
    "\n",
    "X_tr_p = pre.fit_transform(X_train)\n",
    "X_te_p = pre.transform(X_test)\n",
    "feat_names = np.concatenate([\n",
    "    pre.named_transformers_['num'].get_feature_names_out(numeric_cols),\n",
    "    pre.named_transformers_['cat'].get_feature_names_out(['surface', 'highway', 'classification'])\n",
    "])\n",
    "\n",
    "X_tr_df = pd.DataFrame(X_tr_p[:, :len(numeric_cols)], columns=numeric_cols)\n",
    "print(\"Correlation matrix for numeric features:\")\n",
    "print(X_tr_df.corr())\n",
    "\n",
    "# === 7. Zero-variance & VIF filtering ===\n",
    "vt = VarianceThreshold(0.0)\n",
    "sup = vt.fit(X_tr_p).get_support()\n",
    "X_tr_v = X_tr_p[:, sup]\n",
    "X_te_v = X_te_p[:, sup]\n",
    "fv = feat_names[sup]\n",
    "vifs = []\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    for i in range(X_tr_v.shape[1]):\n",
    "        try: vifs.append(variance_inflation_factor(X_tr_v, i))\n",
    "        except:  vifs.append(np.inf)\n",
    "vif_df = pd.DataFrame({'feature': fv, 'VIF': vifs})\n",
    "# === Common output ===\n",
    "vif_df.to_csv(output_folder_common/'vif_results.csv', index=False)\n",
    "keep = vif_df[vif_df['VIF'] <= 5]['feature'].tolist()\n",
    "must_include = ['maxspeed', 'maxspeed_sq', 'aadt_speed']\n",
    "for f in must_include:\n",
    "    if f in fv and f not in keep:\n",
    "        keep.append(f)\n",
    "keep_idx = [list(fv).index(f) for f in keep]\n",
    "X_tr_f = X_tr_v[:, keep_idx]\n",
    "X_te_f = X_te_v[:, keep_idx]\n",
    "\n",
    "# === 8. Train Random Forest ===\n",
    "rf = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "rf.fit(X_tr_f, y_tr_rate)\n",
    "rf_importances = pd.DataFrame({'feature':keep,'importance':rf.feature_importances_})\\\n",
    "  .sort_values('importance',ascending=False)\n",
    "# === Common output ===\n",
    "rf_importances.to_csv(output_folder_common/'rf_importances.csv',index=False)\n",
    "\n",
    "# === 9. Train Poisson GLM ===\n",
    "design = pd.DataFrame(X_tr_f,columns=keep,index=X_train.index)\n",
    "design_c = sm.add_constant(design,has_constant='add')\n",
    "offset = np.log(X_train['segment_length_km'])\n",
    "pois_res = sm.GLM(y_tr_crash,design_c,sm.families.Poisson(),offset=offset).fit()\n",
    "# === Common output ===\n",
    "pd.DataFrame({'IRR':np.exp(pois_res.params)}).to_csv(output_folder_common/'poisson_irrs.csv')\n",
    "with open(output_folder_common/'poisson_metrics.txt','w') as f: f.write(f\"AIC:{pois_res.aic}\\nDeviance:{pois_res.deviance}\\n\")\n",
    "\n",
    "# === 10. Evaluate Random Forest ===\n",
    "pred_rf = rf.predict(X_te_f)\n",
    "with open(output_folder_common/'rf_metrics.txt','w') as f:\n",
    "    f.write(f\"MAE:{mean_absolute_error(y_te_rate,pred_rf)}\\nRMSE:{root_mean_squared_error(y_te_rate,pred_rf)}\\nR2:{r2_score(y_te_rate,pred_rf)}\\n\")\n",
    "pd.Series(-cross_val_score(rf,np.vstack([X_tr_f,X_te_f]),np.hstack([y_tr_rate,y_te_rate]),cv=5,scoring='neg_root_mean_squared_error')).to_csv(output_folder_common/'rf_cv_rmse.csv',index=False)\n",
    "\n",
    "# === 11. Diagnostics plots ===\n",
    "plt.scatter(pred_rf,y_te_rate-pred_rf);plt.xlabel('Fitted');plt.ylabel('Residuals');plt.title('RF Resids vs Fitted');plt.savefig(output_folder_common/'rf_residuals.png');plt.clf()\n",
    "pred_po = pois_res.predict(sm.add_constant(pd.DataFrame(X_te_f,columns=keep),has_constant='add'),offset=np.log(X_test['segment_length_km']))\n",
    "plt.scatter(pred_po,y_te_crash-pred_po);plt.xlabel('Fitted');plt.ylabel('Residuals');plt.title('Poisson Resids vs Fitted');plt.savefig(output_folder_common/'poisson_residuals.png');plt.clf()\n",
    "\n",
    "# === 12. Speed-based multi-target recommendations ===\n",
    "reduction_percs = list(range(10,110,10))\n",
    "all_recs = {f\"rf_rec_{p}pct\":[] for p in reduction_percs}\n",
    "for idx,row in X_test.iterrows():\n",
    "    static = pre.transform(pd.DataFrame([row]))[:,sup][:,keep_idx]\n",
    "    base = rf.predict(static.reshape(1,-1))[0]\n",
    "    for p in reduction_percs:\n",
    "        target=(1-p/100)*base\n",
    "        def f(s):\n",
    "            tmp=row.copy();tmp['maxspeed']=s\n",
    "            arr=pre.transform(pd.DataFrame([tmp]))[:,sup][:,keep_idx]\n",
    "            return rf.predict(arr.reshape(1,-1))[0]-target\n",
    "        try: rec=brentq(f,s_min,s_max)\n",
    "        except: rec=np.nan\n",
    "        all_recs[f\"rf_rec_{p}pct\"].append(rec)\n",
    "rec_df = pd.DataFrame({\n",
    "    'osm_id': X_test['osm_id'],\n",
    "    'existing_maxspeed': X_test['maxspeed'],\n",
    "    'highway': X_test['highway'],\n",
    "    'classification': X_test['classification']\n",
    "})\n",
    "for p in reduction_percs:\n",
    "    rec_df[f\"rf_rec_{p}pct\"] = all_recs[f\"rf_rec_{p}pct\"]\n",
    "# === Speed-based output ===\n",
    "rec_df.to_csv(output_folder_speed/'recommendations.csv', index=False)\n",
    "\n",
    "# === 13. Speed-based comparative analysis ===\n",
    "for p in reduction_percs:\n",
    "    rec_df[f\"gap_{p}pct\"] = rec_df[f\"rf_rec_{p}pct\"] - rec_df['existing_maxspeed']\n",
    "gap_summaries=[]\n",
    "for p in reduction_percs:\n",
    "    temp=rec_df[['highway','classification',f'gap_{p}pct']]\n",
    "    agg=temp.groupby(['highway','classification'])[f'gap_{p}pct'].agg(\n",
    "        mean_gap='mean',median_gap='median',\n",
    "        pct_within_5=lambda x:(x.abs()<=5).mean(),\n",
    "        pct_within_10=lambda x:(x.abs()<=10).mean()\n",
    "    ).reset_index()\n",
    "    agg['reduction_pct']=p\n",
    "    gap_summaries.append(agg)\n",
    "gap_summary_all=pd.concat(gap_summaries,ignore_index=True)\n",
    "# === Speed-based output ===\n",
    "gap_summary_all.to_csv(output_folder_speed/'gap_summary_all.csv',index=False)\n",
    "\n",
    "# ===== POLICY-RELEVANT SENSITIVITY SCENARIOS =====\n",
    "scenarios = {\n",
    "    # AADT (traffic volume)\n",
    "    'aadt_up_10': 1.10,         # +10% traffic\n",
    "    'aadt_down_10': 0.90,       # -10% traffic\n",
    "    'aadt_up_20': 1.20,         # +20% traffic\n",
    "    'aadt_down_20': 0.80,       # -20% traffic\n",
    "\n",
    "    # Curvature (road design)\n",
    "    'curvature_up_10': 1.10,    # +10% more curvy\n",
    "    'curvature_down_10': 0.90,  # -10% less curvy\n",
    "\n",
    "    # Population (area densification)\n",
    "    'population_up_10': 1.10,   # +10% pop\n",
    "    'population_down_10': 0.90, # -10% pop\n",
    "\n",
    "    # Lanes (infrastructure change, handled as int addition)\n",
    "    'lanes_up_1': 1,            # +1 lane\n",
    "    'lanes_down_1': -1,         # -1 lane\n",
    "\n",
    "    # Speed limit (policy change)\n",
    "    'maxspeed_up_10': 10,       # +10 kph\n",
    "    'maxspeed_down_10': -10,    # -10 kph\n",
    "\n",
    "    # Segment length (road works, splitting/merging)\n",
    "    'segment_length_up_10': 1.10,\n",
    "    'segment_length_down_10': 0.90,\n",
    "}\n",
    "\n",
    "# === 14. Speed-based sensitivity analysis ===\n",
    "try:\n",
    "    sa_list=[]\n",
    "    for name, factor in scenarios.items():\n",
    "        tmp = X_test.copy()\n",
    "        # Apply the scenario\n",
    "        if 'aadt' in name:\n",
    "            tmp['aadt'] *= factor\n",
    "        elif 'curvature' in name:\n",
    "            tmp['mean_curvature'] *= factor\n",
    "        elif 'population' in name:\n",
    "            tmp['population'] *= factor\n",
    "        elif 'segment_length' in name:\n",
    "            tmp['segment_length_km'] *= factor\n",
    "        elif name == 'lanes_up_1':\n",
    "            tmp['lanes'] = tmp['lanes'] + 1\n",
    "        elif name == 'lanes_down_1':\n",
    "            tmp['lanes'] = np.maximum(tmp['lanes'] - 1, 1)\n",
    "        elif name == 'maxspeed_up_10':\n",
    "            tmp['maxspeed'] = tmp['maxspeed'] + 10\n",
    "        elif name == 'maxspeed_down_10':\n",
    "            tmp['maxspeed'] = np.maximum(tmp['maxspeed'] - 10, 10)\n",
    "\n",
    "        # Recompute all feature-engineered columns affected by changes:\n",
    "        tmp['aadt_curv_interaction'] = tmp['aadt'] * tmp['mean_curvature']\n",
    "        tmp['aadt_lanes_interaction'] = tmp['aadt'] * tmp['lanes']\n",
    "        tmp['aadt_length_interaction'] = tmp['aadt'] * tmp['segment_length_km']\n",
    "        tmp['aadt_speed'] = tmp['aadt'] * tmp['maxspeed']\n",
    "        tmp['aadt_pop_interaction'] = tmp['aadt'] * tmp['population']\n",
    "        tmp['lanes_length_interaction'] = tmp['lanes'] * tmp['segment_length_km']\n",
    "        tmp['lanes_speed_interaction'] = tmp['lanes'] * tmp['maxspeed']\n",
    "        tmp['curv_lanes_interaction'] = tmp['mean_curvature'] * tmp['lanes']\n",
    "        tmp['maxspeed_sq'] = tmp['maxspeed'] ** 2\n",
    "        tmp['mean_curvature_sq'] = tmp['mean_curvature'] ** 2\n",
    "        tmp['aadt_sq'] = tmp['aadt'] ** 2\n",
    "        tmp['lanes_sq'] = tmp['lanes'] ** 2\n",
    "        tmp['segment_length_sq'] = tmp['segment_length_km'] ** 2\n",
    "        tmp['aadt_per_lane'] = tmp['aadt'] / tmp['lanes'].replace(0, np.nan)\n",
    "        tmp['pop_density_per_length'] = tmp['population'] / tmp['segment_length_km'].replace(0, np.nan)\n",
    "        tmp['aadt_per_length'] = tmp['aadt'] / tmp['segment_length_km'].replace(0, np.nan)\n",
    "        tmp['urban'] = (tmp['classification'].str.lower().str.contains('urban')).astype(int)\n",
    "        tmp['urban_high_aadt'] = ((tmp['urban'] == 1) & (tmp['aadt'] > tmp['aadt'].median())).astype(int)\n",
    "        tmp['bridge_high_speed'] = ((tmp['bridge'] == 1) & (tmp['maxspeed'] > tmp['maxspeed'].median())).astype(int)\n",
    "        tmp['tunnel_high_speed'] = ((tmp['tunnel'] == 1) & (tmp['maxspeed'] > tmp['maxspeed'].median())).astype(int)\n",
    "        tmp['sidewalk_high_pop'] = ((tmp['sidewalk'] == 1) & (tmp['population'] > tmp['population'].median())).astype(int)\n",
    "        tmp['oneway_high_speed'] = ((tmp['oneway'] == 1) & (tmp['maxspeed'] > tmp['maxspeed'].median())).astype(int)\n",
    "        if 'crossings' in tmp.columns:\n",
    "            tmp['crossings_per_km'] = tmp['crossings'] / tmp['segment_length_km'].replace(0, np.nan)\n",
    "\n",
    "        # Now do the rest as before:\n",
    "        static_all = pre.transform(tmp)[:,sup][:,keep_idx]\n",
    "        base_all = rf.predict(static_all)\n",
    "        for p in reduction_percs:\n",
    "            target = 0.01*(100-p)*base_all\n",
    "            recs = []\n",
    "            for row, t in zip(tmp.to_dict('records'), target):\n",
    "                def f(s):\n",
    "                    trow = row.copy(); trow['maxspeed'] = s\n",
    "                    arr = pre.transform(pd.DataFrame([trow]))[:,sup][:,keep_idx]\n",
    "                    return rf.predict(arr.reshape(1,-1))[0] - t\n",
    "                try:\n",
    "                    recs.append(brentq(f,s_min,s_max))\n",
    "                except:\n",
    "                    recs.append(np.nan)\n",
    "            sa_list.append(pd.DataFrame({\n",
    "                'osm_id':X_test['osm_id'],'scenario':name,\n",
    "                'reduction_pct':p,'rec':recs\n",
    "            }))\n",
    "    sa_df = pd.concat(sa_list,ignore_index=True)\n",
    "    # === Speed-based output ===\n",
    "    sa_df.to_csv(output_folder_speed/'sensitivity_analysis_all.csv',index=False)\n",
    "except Exception as e:\n",
    "    print(\"Sensitivity analysis not run:\", e)\n",
    "\n",
    "\n",
    "# === 15. Save models ===\n",
    "with open(output_folder_common/'rf_model.pkl','wb') as f: pickle.dump(rf,f)\n",
    "with open(output_folder_common/'poisson_model.pkl','wb') as f: pickle.dump(pois_res,f)\n",
    "\n",
    "# === 16. Other-features-based recommendations and risk scoring ===\n",
    "# Identify top 3 non-speed features\n",
    "top_non_speed_features = rf_importances[\n",
    "    ~rf_importances['feature'].str.contains('maxspeed')\n",
    "]['feature'].head(3).tolist()\n",
    "print(\"Top non-speed features:\", top_non_speed_features)\n",
    "\n",
    "# Risk scoring and flagging\n",
    "scaler = MinMaxScaler()\n",
    "X_test_scaled = X_test.copy()\n",
    "X_test_scaled[[f + '_norm' for f in top_non_speed_features]] = scaler.fit_transform(X_test[top_non_speed_features])\n",
    "X_test_scaled['composite_risk'] = X_test_scaled[[f + '_norm' for f in top_non_speed_features]].sum(axis=1)\n",
    "X_test_scaled['priority'] = X_test_scaled['composite_risk'] > X_test_scaled['composite_risk'].quantile(0.9)\n",
    "\n",
    "# Export segments with highest composite risk (non-speed-based)\n",
    "X_test_scaled[['osm_id'] + top_non_speed_features + ['composite_risk', 'priority']].to_csv(\n",
    "    output_folder_other/'high_risk_segments.csv', index=False)\n",
    "\n",
    "# Also create recommendations as text, based on which feature is highest\n",
    "def get_other_recommendation(row):\n",
    "    best = np.argmax([row[f + '_norm'] for f in top_non_speed_features])\n",
    "    if best == 0:\n",
    "        return f\"Intervene due to high {top_non_speed_features[0]}\"\n",
    "    elif best == 1:\n",
    "        return f\"Intervene due to high {top_non_speed_features[1]}\"\n",
    "    else:\n",
    "        return f\"Intervene due to high {top_non_speed_features[2]}\"\n",
    "\n",
    "X_test_scaled['other_feature_recommendation'] = X_test_scaled.apply(get_other_recommendation, axis=1)\n",
    "X_test_scaled[['osm_id', 'composite_risk', 'priority', 'other_feature_recommendation']].to_csv(\n",
    "    output_folder_other/'recommendations_by_other_features.csv', index=False)\n",
    "\n",
    "print(\"Pipeline complete: common, speed-based, and other-features-based outputs generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8480ae-b09a-4efc-a913-809e23d27e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
